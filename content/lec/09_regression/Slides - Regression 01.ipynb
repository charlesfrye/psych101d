{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"../../shared/img/slides_banner.svg\" width=2560></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Regression 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from shared.src import quiet\n",
    "from shared.src import seed\n",
    "from shared.src import style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "from IPython.display import HTML, Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc3 as pm\n",
    "import seaborn as sns\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "sns.set_context(\"notebook\", font_scale=1.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import shared.src.utils.util as shared_util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Over the past few weeks, we've focused on inference in models with categorical independent variables and continuous dependent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For example, how does the performance of subjects on a working memory task depend on their age and the difficulty of the task?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# This approach is very general, because we can turn anything into a category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For the first part of this lecture, we'll work with a famous dataset:\n",
    "Sir Francis Galton's parent-child height dataset ([source](https://doi.org/10.7910/DVN/T0HSJ1)),\n",
    "on which the technique of regression was named and invented\n",
    "([original paper](http://www.stat.ucla.edu/~nchristo/statistics100C/history_regression.pdf))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/galton_height.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "It contains the heights of a nearly 1000 English individuals, their sex, and the height of both their parents, collected in 1885.\n",
    "\n",
    "Following Galton, we summarize the parental heights by averaging them to obtain a \"`midparental_height`\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Galton did not have access to `matplotlib`, so he took stock of his data by means of a table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"img/galton_table.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The table in the center is the joint distribution.\n",
    "The horizontal axis of the table (aka the columns)\n",
    "is the adult height of the children,\n",
    "while the vertical axis (aka the rows)\n",
    "is the midparental height.\n",
    "\n",
    "The totals within each column and row--the\n",
    "marginal, unnormalized histograms--\n",
    "are indicated in the second row from the bottom (**Totals**)\n",
    "and in the third column from the right (**Total Number of Mid-parents**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Note: Galton didn't have `matplotlib` or Python,\n",
    "but he did have a \"computer\":\n",
    "a clerk who did all of his calculations for him!\n",
    "\n",
    "He also wasn't able to make a kernel density estimate,\n",
    "but he did the next best thing:\n",
    "he averaged the values in each cell with its four neighbors,\n",
    "which he referred to as \"smoothing\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We might make a similar summary by means of `jointplot`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sns.jointplot(x=\"midparental_height\", y=\"height\",  data=df, kind=\"hex\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Unlike Galton, we don't modify the heights of the female children,\n",
    "so our results are slightly different.\n",
    "\n",
    "Notice the \"marginal\" distributions appear in roughly the same place in this plot\n",
    "as they do in Galton's table!\n",
    "_Plus ça change_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It seems like the child's height can be predicted from the parents' height:\n",
    "if the parents are taller than average, the child is also taller than average.\n",
    "\n",
    "But how can we quantify this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If the data were categorical, we'd know what to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## We can redefine numerical variables as categorical variables by binning them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's think of the bins as group labels:\n",
    "if I am in the bin of heights between 64 and 66 inches,\n",
    "then I am in the \"group\" of \"64-66 inches\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def categorize_height(height):\n",
    "    if height <= 64:\n",
    "        return \"<64\"\n",
    "    if height <= 66:\n",
    "        return \"64-66\"\n",
    "    if height <= 68:\n",
    "        return \"66-68\"\n",
    "    if height <= 70:\n",
    "        return \"68-70\"\n",
    "    return \">70\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "This function converts a \"numerical height\" into a \"categorical height\":\n",
    "the inputs are numbers and the outputs are strings indicating to which bin the numerical height belonged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df[\"midparental_height_category\"] = df[\"midparental_height\"].apply(\n",
    "    categorize_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(12, 6))\n",
    "sns.pointplot(x=\"midparental_height_category\", y=\"height\", data=df, ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "If we visualize the resulting data with one of our categorical visualizations,\n",
    "like the `pointplot` or the `violinplot`,\n",
    "we can apply the techniques we developed for other categorical problems.\n",
    "\n",
    "Do the means appear different across the levles of the midparental height variable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For convenience,\n",
    "we'd like to work with indices:\n",
    "category labels that are numbers beginning at 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "category_to_idx = {\n",
    "    \"<64\"   : 0,\n",
    "    \"64-66\" : 1,\n",
    "    \"66-68\" : 2,\n",
    "    \"68-70\" : 3,\n",
    "    \">70\"   : 4}\n",
    "\n",
    "df[\"midparental_height_category_idx\"] = df[\"midparental_height_category\"].map(category_to_idx)\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "This cell uses an alternative to `apply` called `map`.\n",
    "The result is similarly a `Series` with transformed values.\n",
    "\n",
    "In addition to taking in a function,\n",
    "[`map`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.map.html)\n",
    "can take in a dictionary.\n",
    "Each entry in the calling `Series` is passed as a key to the dictionary,\n",
    "and the resulting value becomes the entry in the output `Series`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(12, 6))\n",
    "sns.stripplot(x=\"midparental_height_category_idx\", y=\"height\", data=df, ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "A `stripplot` is a convenient, easy-to-read alternative to the `violinplot`.\n",
    "It still displays the entire distribution of the data,\n",
    "not just summary statistics,\n",
    "but it doesn't do any smoothing.\n",
    "All of the data from a given group are plotted as a \"strip\".\n",
    "In this case, the strips are vertical.\n",
    "Within a strip, the horizontal position is meaningless:\n",
    "points are simply \"jittered\" in order to make the density easier to see.\n",
    "\n",
    "It is analogous to the `rugplot`,\n",
    "but more useful for cotintuous data that also varies along a categorical variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "As a reminder:\n",
    "in the language of mixture distributions,\n",
    "we are thinking of the marginal distribution of individuals' heights\n",
    "as a mixture of distributions, one for each `midparental_height_category`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(figsize=(12, 12), nrows=2, sharex=True, sharey=True)\n",
    "sns.distplot(df[\"height\"], color=\"k\", ax=axs[0], label=\"Marginal\", axlabel=\"\"); axs[0].legend();\n",
    "[sns.distplot(df[\"height\"][selector], ax=axs[1])\n",
    " for selector in [df[\"midparental_height_category_idx\"]  == idx for idx in range(5)]];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Now that we have a categorical variable, we can use our categorical effect modeling tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "At this point,\n",
    "writing down a model like this should be straightforward for you.\n",
    "\n",
    "Make sure it's clear why we've chosen each of the components,\n",
    "which parts are the prior and which are the likelihood,\n",
    "and which parts we'll get a posterior over when we sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "\\text{group_means} \\sim \\text{Normal}(\\mu_g, 1e3, \\text{shape}=5)\\\\\n",
    "\\sigma \\sim \\text{Exponential}\\left(\\frac{1}{2 \\sigma_p}\\right)\\\\\n",
    "\\text{height} \\sim \\text{Normal}\\left(\\text{group_means}[i], \\sigma\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Where $\\mu_g$ is the grand mean and\n",
    "$\\sigma_p$ is the pooled standard deviation,\n",
    "as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "grand_mean = df.groupby(\"midparental_height_category_idx\")[\"height\"].mean().mean()\n",
    "pooled_sd = df.groupby(\"midparental_height_category_idx\")[\"height\"].std().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as categorical_model:\n",
    "    group_means = pm.Normal(\"mus\", mu=grand_mean, sd=1e3, shape=5)\n",
    "    sd = pm.Exponential(\"sigma\", lam=1 / pooled_sd)\n",
    "    \n",
    "    heights = pm.Normal(\"heights\",\n",
    "                        mu=group_means[df[\"midparental_height_category_idx\"]],\n",
    "                        sd=sd,\n",
    "                        observed=df[\"height\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with categorical_model:\n",
    "    categorical_trace = pm.sample(tune=1000)\n",
    "\n",
    "categorical_posterior_df = shared_util.samples_to_dataframe(categorical_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pm.plot_posterior(categorical_trace, figsize=(12, 18), text_size=16,\n",
    "                  ref_val=grand_mean, varnames=[\"mus\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(12, 6))\n",
    "sns.pointplot(x=\"midparental_height_category_idx\", y=\"height\", data=df, ax=ax);\n",
    "[plt.plot(sample[\"mus\"], color=\"C0\", alpha=0.05)\n",
    " for _, sample in categorical_posterior_df.iloc[:100, :].iterrows()];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In the terms of the models we've built so far,\n",
    "there appears to be \"an effect\" of `midparental_height_category`\n",
    "on the `height` of the child:\n",
    "the means of the heights of the children in each category of midparental height\n",
    "appear to differ.\n",
    "Think about how you might quantify this.\n",
    "\n",
    "Note that we can't say anything more cogent about the relationship,\n",
    "because the categorical index is arbitrary -- more on that below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## With this model in hand, we can make predictions about the height of children from their mid-parental height."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "By \"prediction\" I don't necessarily mean \"statement about the future\"\n",
    "-- though in this case, you could perhaps predict the height of a pair's children before they are born.\n",
    "\n",
    "Instead, in statistical modeling we can call a statement about any unknown quantity a \"prediction\",\n",
    "not just quantities that are unknown because they are realized in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Other roughly equivalent terms you might use here include \"guess\" and \"estimate\".\n",
    "\n",
    "A good guess/estimate/prediction is \"close\", in some sense, to the correct answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In regression models,\n",
    "prediction/estimation is often more important than effect detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## In a categorical model with a Normal likelihood, the mean is used as the predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\text{height} \\approx \\text{group_means}[i]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "where $\\approx$ is pronounced \"is approximately equal to\".\n",
    "It means here something like \"is close to\", in some loose sense.\n",
    "\n",
    "If our predictions are good, then our predictions are typically close to the true values.\n",
    "\n",
    "Note that this is different from $\\sim$, the symbol we use to mean that the variable on the left side\n",
    "has the distribution on the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def predict_height_categorical(midparental_height, parameters):\n",
    "    means = parameters[\"mus\"]\n",
    "    midparental_height_category = categorize_height(midparental_height)\n",
    "    midparental_height_idx = category_to_idx[midparental_height_category]\n",
    "    group_mean = means[midparental_height_idx]\n",
    "    \n",
    "    return group_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "predict_height_categorical(66, categorical_posterior_df.iloc[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# But for data where numerical values are meaningful, categories are unnatural."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In truly categorical data,\n",
    "the order of the values doesn't matter.\n",
    "\n",
    "Perhaps we might be interested in whether\n",
    "there are differences in the rate at which cars of different colors get tickets.\n",
    "Colors don't have a natural order to them, so it makes sense to treat color as a category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But if we were interested in whether\n",
    "the rate of getting tickets is dependent on the speed a car is traveling,\n",
    "the situation is very different:\n",
    "we have reason to believe there is a _natural order_ to speeds,\n",
    "from low to high, and that the relationship is simplest in terms of this natural order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Do demonstrate this point,\n",
    "let's look at the midparental height data\n",
    "with the category indices reordered.\n",
    "\n",
    "Changing the order of the categories\n",
    "shouldn't affect the analysis of categorical data,\n",
    "since our indices are arbitrary,\n",
    "but for this data, the picture that emerges is very different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "new_order = [4, 2, 3, 1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(12, 6))\n",
    "sns.pointplot(x=\"midparental_height_category_idx\", y=\"height\", data=df, ax=ax,\n",
    "              order=new_order);\n",
    "[plt.plot(sample[\"mus\"][new_order], color=\"C0\", alpha=0.05)\n",
    " for _, sample in categorical_posterior_df.iloc[:100, :].iterrows()];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The relationship between midparental height and the height of the child now looks complicated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# If we instead retain the numerical meaning of the independent variable, the relationship often appears simpler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Let's instead plot the categories in the original, natural order,\n",
    "along with the numerical values to which they correspond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(12, 6))\n",
    "sns.pointplot(x=\"midparental_height_category\", y=\"height\", data=df, ax=ax,\n",
    "              order=reversed(df[\"midparental_height_category\"].unique()));\n",
    "[plt.plot(sample[\"mus\"], color=\"C0\", alpha=0.05)\n",
    " for _, sample in categorical_posterior_df.iloc[:100, :].iterrows()];\n",
    "# plt.plot([0, 4], [65, 70], color=\"k\", lw=6);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In the right order,\n",
    "there appears to be a relatively simple relationship between\n",
    "midparent height and child height:\n",
    "increasing the midparent height from one category to the next,\n",
    "that is, by about two inches,\n",
    "appears to increase the average height by something slightly less\n",
    "than two inches.\n",
    "This effect appears reasonable consistently across the categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Whenever the effect of changing one variable is an increment of a fixed proportion\n",
    "in another, the relationship between the two is described by a line:\n",
    "it is _linear_.\n",
    "\n",
    "By uncommenting the final line in the above cell,\n",
    "you can plot a straight line over the data\n",
    "that passes close by the average heights of each group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We can plot the same information on top of the raw data, for comparison.\n",
    "It does appear that the line passes\n",
    "close to the middle of each group's values.\n",
    "\n",
    "Furthermore, the groups appear to be fairly clustered around their central values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(12, 6))\n",
    "sns.stripplot(x=\"midparental_height_category_idx\", y=\"height\", data=df, ax=ax);\n",
    "[plt.plot(sample[\"mus\"], color=\"C0\", alpha=0.05)\n",
    " for _, sample in categorical_posterior_df.iloc[:100, :].iterrows()];\n",
    "plt.plot([0, 4], [64, 70], color=\"k\", lw=6);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We can plot that same line over the original data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "g = sns.jointplot(x=\"midparental_height\", y=\"height\", data=df)\n",
    "g.ax_joint.plot([63, 71], [64, 70], color=\"k\", lw=6);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For any given choice of midparental height,\n",
    "this line appears to come close to most of the heights of the children."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# As with the categorical model, we can use this relationship to make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For the categorical model, we had"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\text{height} \\approx \\text{group_means}[i] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Consider the classic formula for a line:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$y = m \\cdot x + b$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "slope = 1.5; intercept = 0.75\n",
    "xs = pd.Series(range(-3, 4))\n",
    "ys = slope * xs + intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(10, 10)); #plt.axis(\"equal\");\n",
    "ax.vlines(0, *[-4, 4], lw=6); ax.hlines(0, *[-4, 4], lw=6);\n",
    "ax.plot(xs, ys, lw=4, label=f\"$y = {slope}\\cdot x + {intercept}$\", zorder=3);\n",
    "ax.set_ylim([-3, 3]); ax.set_xlim([-3, 3]); ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So we write the predicted height for the individual as a function of the midparental height like so:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "\\text{predicted_height} = \\text{slope} \\cdot \\text{midparental_height} + \\text{intercept}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "That is, if we want to guess the child's adult height,\n",
    "we need to multiply their parent's average height by some value and add another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "which is equivalent to saying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "\\text{height} \\approx \\text{slope} \\cdot \\text{midparental_height} + \\text{intercept}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In Python, that looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def predict_height(midparental_heights, slope, intercept):\n",
    "    return slope * midparental_heights + intercept\n",
    "\n",
    "hand_slope, hand_intercept = 0.75, 17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "where `hand_slope` and `hand_intercept` are values picked \"by hand\",\n",
    "(or rather, \"by eye\")\n",
    "to give decent predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The predictions of this model look like so,\n",
    "compared to the original data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "g = sns.jointplot(x=\"midparental_height\", y=\"height\", data=df)\n",
    "g.ax_joint.scatter(\n",
    "    df[\"midparental_height\"],\n",
    "    predict_height(df[\"midparental_height\"], hand_slope, hand_intercept),\n",
    "    color=\"C1\", lw=6);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## There are two big issues with this process:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- we didn't define what we meant by \"$\\approx$\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "This left us with no choice but to pick the parameters of the line by hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- we no longer have a probabilistic model for our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "It's unclear how we might express our uncertainty about the size of our errors\n",
    "or our uncertainty about the parameters of the line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# We solve both of these issues at once by specifying a likelihood.\n",
    "\n",
    "Or, alternatively, we define $\\approx$ in terms of a distribution, $\\sim$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "\\text{height} \\sim \\text{Normal}\\left(\\text{predicted_height}, \\sigma\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "\\text{height} \\sim \\text{Normal}\\left(\\text{slope} \\cdot \\text{midparental_height} + \\text{intercept}, \\sigma\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "That is, if our predictions are good,\n",
    "we expect that\n",
    "- we are most likely to observe `height`s close to our `predicted_height`\n",
    "- we will observe `height`s both above and below our prediction in equal proportion\n",
    "- the average `height` for children of parents with a given `midparental_height` is equal to our `predicted_height`\n",
    "- the spread of observed `height`s around our `predicted_height` is given by $\\sigma$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Meaning of intercept: what is predicted height of a child whose parent heights average to 0 inches?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Meaning of slope: if the parents' average height increases by 1 inch, by how many inches (and in which direction!) should I change my prediction of the child's height?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This gives us a likelihood for our data,\n",
    "and we just need to define a prior over the parameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "\\text{slope} \\sim \\text{Normal}(0, 1)\\\\\n",
    "\\text{intercept} \\sim \\text{Normal}(0, 10)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The prior on the slope means that we expect the heights of children to change by somewhere between ±3 inches\n",
    "for each inch of change in midparental height.\n",
    "\n",
    "The prior on the intercept means that we expect a child whose parents average to 0 inches tall\n",
    "to be between -30 and 30 inches tall.\n",
    "\n",
    "These priors are possibly too loose\n",
    "(we might suspect that heights are more likely to increase than decrease,\n",
    "that they don't increase by more than one inch per parental inch,\n",
    "etc.),\n",
    "but let's work with them for now.\n",
    "\n",
    "Think about how you might incoroporate those beliefs into a prior,\n",
    "and try them out in the model below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "\\sigma \\sim \\text{Exponential}\\left(1 / \\sigma_{h}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "height_sd = df[\"height\"].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "As usually, we select a weak, overestimating prior for the standard deviation.\n",
    "\n",
    "Here, we use the standard deviation of the height variable, $\\sigma_h$,\n",
    "aka `height_sd`,\n",
    "as the mean for the prior for the standard deviation of our errors.\n",
    "\n",
    "Since our predictions can't increase this variability,\n",
    "the value $\\sigma_h$ is something of an upper bound."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "That done, we've defined prior distributions for all of the random variable components of the likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "\\text{height} \\sim \\text{Normal}\\left(\\text{slope} \\cdot \\text{midparental_height} + \\text{intercept}, \\sigma\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Once we have a likelihood and a prior, we can use `pyMC` to define a model and sample from its posterior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as linear_model:\n",
    "    intercept = pm.Normal(\"intercept\", mu=0, sd=1e1)\n",
    "    slope = pm.Normal(\"slope\", mu=0, sd=1)\n",
    "    \n",
    "    sigma = pm.Exponential(\"sigma\", lam=1 / height_sd)\n",
    "    \n",
    "    height = pm.Normal(\"heights\",\n",
    "                       mu=slope * df[\"midparental_height\"] + intercept,\n",
    "                       sd=sigma,\n",
    "                       observed=df[\"height\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with linear_model:\n",
    "    linear_trace = pm.sample(tune=1000)\n",
    "    \n",
    "linear_posterior_df = shared_util.samples_to_dataframe(linear_trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The resulting posterior represents our uncertainty about the true values of the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "As always, the posterior has two pieces:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "\\color{green}{p(\\text{slope}, \\text{intercept}, \\sigma \\vert \\text{data})}\n",
    "\\propto \\color{darkgoldenrod}{p(\\text{data} \\vert \\text{slope}, \\text{intercept}, \\sigma)}\n",
    "\\cdot \\color{darkblue}{p(\\text{slope}, \\text{intercept}, \\sigma)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "That is, our\n",
    "\n",
    "$\\color{green}{\\text{updated belief about the plausibility of a setting of the parameters}}$\n",
    "\n",
    "is proportional to\n",
    "\n",
    "$\\color{darkgoldenrod}{\\text{how likely the data is with that choice of the parameters}}$\n",
    "\n",
    "multiplied by\n",
    "\n",
    "$\\color{darkblue}{\\text{how plausible we thought those parameters were before we saw the data}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## For a `Normal` distribution, the likelihood is driven by how close the data is to the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(12, 6))\n",
    "xs = np.linspace(-3, 3)\n",
    "ax.plot(xs, scipy.stats.norm(0, 1).pdf(xs), lw=6);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The `Normal` likelihood is simple:\n",
    "there are no extra bumps or other strange shapes;\n",
    "the distribution is symmetric,\n",
    "so it doesn't matter where you're above or below the center.\n",
    "The closer the value is to the mean (0 in the plot above),\n",
    "the greater its probability under a `Normal` distribution.\n",
    "No other information is needed.\n",
    "\n",
    "Therefore our likelihood term will push the model to predict a `height` as close as possible\n",
    "to the value observed.\n",
    "\n",
    "Because we restricted it to making predictions using a line,\n",
    "the model will try to find lines\n",
    "whose $y$-values pass as close as possible to the observed `height`s.\n",
    "Our prior further indicates that it should prefer lines whose\n",
    "slope and intercept are not too large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pm.plot_posterior(linear_trace, figsize=(12, 12), text_size=16,\n",
    "                  ref_val=[0, 0, height_sd]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The fact that the `slope` is far away from 0\n",
    "indicates that the height of the parents is useful for\n",
    "predicting the height of the child in adulthood.\n",
    "\n",
    "If the slope were sometimes positive, sometimes negative,\n",
    "then we would suspect that it is not useful for prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## From a single sample, we can produce predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We define a quick function for predicting heights using parameters in the format of our posterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def predict_height_from_parameters(midparent_heights, parameters):\n",
    "    return predict_height(midparent_heights, parameters[\"slope\"], parameters[\"intercept\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "predict_height_from_parameters(0, linear_trace[0]), linear_trace[0][\"intercept\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "predict_height_from_parameters(1, linear_trace[0]) - linear_trace[0][\"intercept\"], linear_trace[0][\"slope\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "predict_height_from_parameters(0, linear_trace[-1]), linear_trace[0][\"intercept\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "xs = np.linspace(60, 75)  # evenly-spaced numbers between 60 and 75\n",
    "g = sns.jointplot(y=\"height\", x=\"midparental_height\", data=df);\n",
    "g.ax_joint.plot(xs, predict_height_from_parameters(xs, linear_trace[0]), lw=4, color=\"C1\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The standard deviation parameter indicates the expected variability in observed values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "xs = np.linspace(60, 75)  # evenly-spaced numbers between 60 and 75\n",
    "g = sns.jointplot(y=\"height\", x=\"midparental_height\", data=df);\n",
    "g.ax_joint.plot(xs, predict_height_from_parameters(xs, linear_trace[0]), lw=4, color=\"C1\");\n",
    "g.ax_joint.errorbar(xs,\n",
    "                    predict_height_from_parameters(xs, linear_trace[0]),\n",
    "                    yerr=2 * linear_trace[0][\"sigma\"],\n",
    "                    lw=5, color=\"C1\", alpha=0.3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The transparent gold background here represents\n",
    "the middle 95% of the likelihood component of this sample from the posterior,\n",
    "overlaid on top of the data and underneath the predictions according to this sample's parameters.\n",
    "We should expect about 95% of observations to fall inside this region.\n",
    "\n",
    "It is obtained by taking the value of `sigma` and multiplying it by two--\n",
    "the middle four standard deviations of a Normal cover 95% of the distribution \n",
    "--and plotting it using `errorbar` to get an approximation to the true shape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "But a single sample isn't necessarily representative,\n",
    "nor does it indicate anything about our uncertainty in the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## If we combine predictions across many of our samples, we can get a sense of our uncertainty in what the correct prediction is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "This is a separate issue from our uncertainty in the values, given the prediction, as plotted above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "posterior_samples = linear_posterior_df.sample(n=100)\n",
    "xs = np.linspace(60, 75)  # evenly-spaced numbers between 64 and 75\n",
    "g = sns.jointplot(y=\"height\", x=\"midparental_height\", data=df);\n",
    "[g.ax_joint.plot(xs, predict_height_from_parameters(xs, sample), lw=4, color=\"C1\", alpha=0.05)\n",
    " for _, sample in posterior_samples.iterrows()];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# In regression, the goal is typically prediction, rather than hypothesis testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "That is, we want to get high quality guesses for the values of the \"$y$-variable\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The presumption is that if we can guess the values of the $y$-variable well\n",
    "based only on the value of the $x$-variable\n",
    "for the data we measured,\n",
    "then in the future if we can only measure $x$,\n",
    "we can use our model to obtain a good guess for $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## To obtain our _best guess_ about the true value of the parameters, we use MAP inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "That is, we select the parameters with the highest probability under the posterior.\n",
    "\n",
    "That is, we maximize the value on the left-hand side of the equation below\n",
    "by changing the values of slope and intercept and calculating the value on the right-hand side."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "$$\n",
    "\\color{green}{p(\\text{slope}, \\text{intercept} \\vert \\text{data})}\n",
    "\\propto \\color{darkgoldenrod}{p(\\text{data} \\vert \\text{slope}, \\text{intercept})}\n",
    "\\cdot \\color{darkblue}{p(\\text{slope}, \\text{intercept})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "linear_MAP = pm.find_MAP(start=linear_trace[0], model=linear_model)\n",
    "\n",
    "linear_MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "posterior_samples = linear_posterior_df.sample(n=100)\n",
    "xs = np.linspace(60, 75)  # evenly-spaced numbers between 64 and 75\n",
    "g = sns.jointplot(y=\"height\", x=\"midparental_height\", data=df);\n",
    "[g.ax_joint.plot(xs, predict_height_from_parameters(xs, sample), lw=4, color=\"C1\", alpha=0.1)\n",
    " for _, sample in posterior_samples.iterrows()];\n",
    "g.ax_joint.plot(xs, predict_height_from_parameters(xs, linear_MAP), lw=4, color=\"k\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "This plot shows the MAP prediction\n",
    "(the predictions based on the maximum probability parameters a posteriori)\n",
    "in black over the sampled posterior predictions in transparent gold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A model where the parameters of one random variable change continuously as a function of another is known as a _regression_ model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Regression means \"backwards movement\". What does a continuous prediction have to do with backwards movement?\n",
    "\n",
    "This terminology is silly, and comes directly from Galton and his unfortunate historical position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "He invented this technique to answer a question\n",
    "about the inheritance of complex traits,\n",
    "like height,\n",
    "and in particular to explain a phenomenon of great concern to him:\n",
    "the children of tall parents, though taller than average,\n",
    "were usually shorter than their parents.\n",
    "\n",
    "This implied that, after multiple generations,\n",
    "a particularly tall individual's descendants would be no more likely\n",
    "to be tall than the descendants of an individual of average height."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "As a Victorian aristocrat, his opinion of average things was quite low,\n",
    "so he referred to this phenomenon as _regression to mediocrity_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## This shows up in our model as the slope being less than 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The result is that if we start with an individual whose height is far from the average,\n",
    "e.g. 72 inches/six feet or 62 inches/5'2'',\n",
    "then after several generations,\n",
    "presuming they have children\n",
    "without specifically picking mates based on height,\n",
    "then the expected height of their great-great-great-grandchildren is most of the way\n",
    "back to the population average,\n",
    "as the simulation shows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "parameters = linear_MAP\n",
    "original_height = 72; num_generations = 5\n",
    "\n",
    "predicted_height = original_height\n",
    "\n",
    "for _ in range(num_generations):\n",
    "    predicted_height = predict_height_from_parameters(predicted_height, parameters)\n",
    "    print(predicted_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df[\"height\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# If the function relating the two variables is linear, the model is a _linear regression_ model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This model is, with its generalizations,\n",
    "the workhorse of statistics and data science."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We'll spend the next lecture covering it in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# We are not restricted to linear relationships between variables and parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Let's consider the probability that a golf putt goes into a hole."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "This section is based off of a [pyMC translation](https://nbviewer.jupyter.org/github/pymc-devs/pymc3/blob/master/docs/source/notebooks/putting_workflow.ipynb)\n",
    "of an original case study by [Andrew Gelman](https://mc-stan.org/users/documentation/case-studies/golf.html),\n",
    "written for the Stan library, an R equivalent of pyMC.\n",
    "\n",
    "The data is from [_Statistics: A Bayesian Perpsective_, by Donald Berry](https://www.jstor.org/stable/2684909),\n",
    "and represents measurements of the putting outcomes for a number of professional golfers. \n",
    "\n",
    "The original goes into much greater depth and demonstrates some very cool pyMC tricks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "golf = pd.read_csv(\"data/golf.csv\", index_col=0)\n",
    "\n",
    "golf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "If we take a look at this data,\n",
    "we might be able to convince ourselves that a line comes \"close enough\"\n",
    "to predicting the probability of getting the putt in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.set_ylim([0, 1])\n",
    "ax.scatter(golf[\"distance\"], golf[\"successes\"] / golf[\"tries\"]);\n",
    "ax.plot([2, 20], [0.95, 0.05], lw=2, color=\"C1\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "You might quibble with the parameters, but these are just meant as a proof-of-principle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## A line is inadequate to capture the relationship between distance and putt success chance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In order to adapt our original model,\n",
    "it seems like we'd only need to change our likelihood:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "\\text{success} \\sim \\text{Bernoulli}(p=\\text{slope}\\cdot\\text{distance} + \\text{intercept})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "\\text{successes} \\sim \\text{Binomial}(n=\\text{tries}, p=\\text{slope}\\cdot\\text{distance} + \\text{intercept})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as golf_linear:\n",
    "    slope = pm.Normal(\"slope\", mu=0, sd=1)\n",
    "    intercept = pm.Normal(\"intercept\", mu=0, sd=1)\n",
    "    \n",
    "    successes = pm.Binomial(\"successes\",\n",
    "                            n=golf[\"tries\"],\n",
    "                            p=slope * golf[\"distance\"] + intercept,\n",
    "                            observed=golf[\"successes\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "But if we try to sample from this model's posterior,\n",
    "we get an error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    with golf_linear:\n",
    "        golf_linear_trace = pm.sample(target_accept=0.9, draws=1000)\n",
    "except pm.parallel_sampling.ParallelSamplingError:\n",
    "    print(\"Error!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Let's try and understand why this is the case.\n",
    "\n",
    "First, let's plot the line from above again,\n",
    "then determine its slope and intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.set_ylim([-1, 2]); ax.hlines(0, 0, 30); ax.set_xlim(0, 30);\n",
    "ax.scatter(golf[\"distance\"], golf[\"successes\"] / golf[\"tries\"]);\n",
    "ax.plot([2, 20], [0.95, 0.05], lw=4, color=\"C1\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "golf_slope = -0.05\n",
    "golf_intercept = 1.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "If we extend that line past 20 feet,\n",
    "we see that the predicted value goes below 0 --\n",
    "meaning that we predict a _negative_ probability of putting success!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(12, 6))\n",
    "xs = np.linspace(0, 30)  # evenly-spaced numbers between 0 and 30\n",
    "ax.set_ylim([-1, 2]); ax.hlines(0, 0, 30, lw=6); ax.set_xlim(0, 30);\n",
    "ax.scatter(golf[\"distance\"], golf[\"successes\"] / golf[\"tries\"]);\n",
    "ax.plot(xs, xs * golf_slope +  golf_intercept, lw=4, color=\"C1\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "If the output values were restricted to being between 0 and 1,\n",
    "then we'd still be in business."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## We can introduce other functions to our models in order to get other relationships besides linear ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In this case,\n",
    "we introduce a \"squasher\" function\n",
    "to make sure our predictions never go below 0 or above 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "\\text{successes} \\sim \\text{Binomial}(n=\\text{tries}, p=\\text{squash}\\left(\\text{slope}\\cdot\\text{distance} + \\text{intercept}\\right))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The typical choice for a \"squasher\" when we want values between 0 and 1 is the function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def squash(xs):\n",
    "    return 1 / (1 + np.exp(-xs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(12, 6))\n",
    "xs = np.linspace(-5, 5)\n",
    "ax.plot(xs, squash(xs), lw=4);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "It's sometimes called the _sigmoid_ (meaning \"S-shaped\") function\n",
    "or the logistic function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "If we apply this function to our parameters from before,\n",
    "we see that the predictions no longer go outside of the acceptable range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(12, 6))\n",
    "xs = np.linspace(0, 30)  # evenly-spaced numbers between 0 and 30\n",
    "ax.set_ylim([-1, 2]); ax.hlines(0, 0, 30, lw=6); ax.set_xlim(0, 30);\n",
    "ax.scatter(golf[\"distance\"], golf[\"successes\"] / golf[\"tries\"]);\n",
    "ax.plot(xs, xs * golf_slope +  golf_intercept, lw=4, color=\"C1\", label=\"before squashing\");\n",
    "ax.plot(xs, squash(golf_slope * xs + golf_intercept), color=\"C2\", lw=4, label=\"after squashing\");\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The functions have to be provided by `pyMC` in order to work with sampling and MAP inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Technically, you can write your own,\n",
    "but this requires some heavy-duty math work using the guts of pyMC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pymc_squash = pm.math.invlogit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Once that's taken care of, we simply include them inside of our model and we are ready to go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as golf_binomial:\n",
    "    slope = pm.Normal(\"slope\", mu=0, sd=1)\n",
    "    intercept = pm.Normal(\"intercept\", mu=0, sd=10)\n",
    "    \n",
    "    successes = pm.Binomial(\"successes\",\n",
    "                            n=golf[\"tries\"],\n",
    "                            p=pymc_squash(slope * golf[\"distance\"] + intercept),\n",
    "                            observed=golf[\"successes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with golf_binomial:\n",
    "    golf_binomial_trace = pm.sample(target_accept=0.9, draws=1000)\n",
    "    \n",
    "golf_binomial_posterior = shared_util.samples_to_dataframe(golf_binomial_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pm.plot_posterior(golf_binomial_trace, figsize=(12, 6), text_size=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Our tools of MAP inference and posterior visualization transfer directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "golf_MAP = pm.find_MAP(start=golf_binomial_trace[0], model=golf_binomial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We just need to make sure to define our prediction function correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def predict_putt_success(distance, parameters):\n",
    "    return squash(parameters[\"slope\"] * distance + parameters[\"intercept\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This function was already present in our model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "\\text{successes} \\sim \\text{Binomial}(n=\\text{tries}, p=\\text{squash}\\left(\\text{slope}\\cdot\\text{distance} + \\text{intercept}\\right))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(12, 6))\n",
    "xs = np.linspace(0, 30)  # evenly-spaced numbers between 0 and 30\n",
    "MAP_predictions = predict_putt_success(xs, golf_MAP)\n",
    "ax.set_ylim([0, 1.1]); ax.scatter(golf[\"distance\"], golf[\"successes\"] / golf[\"tries\"]);\n",
    "[ax.plot(xs, predict_putt_success(xs, sample), lw=4, color=\"C1\", alpha=0.1)\n",
    " for _, sample in golf_binomial_posterior[::20].iterrows()];\n",
    "ax.plot(xs, MAP_predictions, color=\"k\", lw=4, label=\"MAP Prediction\"); ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Again, the predictions from the posterior samples are plotted in transparent gold\n",
    "underneath the MAP prediction and above the data, in dark blue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Important note:\n",
    "the notion of a good prediction has changed -- it's based on a `Binomial` likelihood,\n",
    "not a `Normal` one.\n",
    "\n",
    "It's still in general good for predictions to be \"close to\" observations,\n",
    "but the notion of distance can change."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
